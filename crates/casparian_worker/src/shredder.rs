//! The Iron Shredder - Multiplexed file splitting engine.
//!
//! Design principles:
//! 1. Single-pass with dynamic promotion (no 2x I/O)
//! 2. LRU file handle cache (max 200 open files)
//! 3. Block-based lineage (10KB blocks, not per-row)
//! 4. Atomic writes via .tmp/ â†’ rename
//! 5. Checkpointing every 100MB for resume capability

use cf_protocol::{LineageBlock, ShardMeta, ShredConfig, ShredResult, ShredStrategy};
use std::collections::{HashMap, VecDeque};
use std::fs::{self, File, OpenOptions};
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::{Path, PathBuf};
use thiserror::Error;

/// Default configuration values
const DEFAULT_MAX_HANDLES: usize = 200;
const DEFAULT_BUFFER_SIZE: usize = 65536; // 64KB
const DEFAULT_PROMOTION_THRESHOLD: u64 = 1000;
const DEFAULT_TOP_N_SHARDS: usize = 5;
const LINEAGE_BLOCK_SIZE: usize = 10240; // 10KB
const CHECKPOINT_INTERVAL_BYTES: u64 = 100_000_000; // 100MB

/// Freezer shard key for rare types
const FREEZER_KEY: &str = "_MISC";

#[derive(Error, Debug)]
pub enum ShredError {
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    #[error("Key extraction failed: {0}")]
    KeyExtraction(String),
    #[error("Too many unique keys ({count}). Consider grouping by prefix.")]
    TooManyKeys { count: usize },
    #[error("Strategy not supported: {0}")]
    UnsupportedStrategy(String),
    #[error("Checkpoint error: {0}")]
    Checkpoint(String),
}

/// Manages an LRU cache of file writers
struct WriterCache {
    writers: HashMap<String, ShardWriter>,
    access_order: VecDeque<String>,
    max_handles: usize,
}

impl WriterCache {
    fn new(max_handles: usize) -> Self {
        Self {
            writers: HashMap::new(),
            access_order: VecDeque::new(),
            max_handles,
        }
    }

    fn get_or_create(
        &mut self,
        key: &str,
        output_dir: &Path,
        header: Option<&str>,
    ) -> Result<&mut ShardWriter, ShredError> {
        // Update access order
        self.access_order.retain(|k| k != key);
        self.access_order.push_back(key.to_string());

        // Evict LRU if at capacity
        while self.writers.len() >= self.max_handles && !self.writers.contains_key(key) {
            if let Some(evict_key) = self.access_order.pop_front() {
                if let Some(mut writer) = self.writers.remove(&evict_key) {
                    writer.flush()?;
                }
            }
        }

        // Create new writer if needed
        if !self.writers.contains_key(key) {
            let writer = ShardWriter::new(key, output_dir, header)?;
            self.writers.insert(key.to_string(), writer);
        }

        Ok(self.writers.get_mut(key).unwrap())
    }

    fn flush_all(&mut self) -> Result<(), ShredError> {
        for writer in self.writers.values_mut() {
            writer.flush()?;
        }
        Ok(())
    }

    fn finalize_all(self, output_dir: &Path) -> Result<Vec<ShardMeta>, ShredError> {
        let mut metas = Vec::new();
        for (_key, writer) in self.writers {
            let meta = writer.finalize(output_dir)?;
            metas.push(meta);
        }
        Ok(metas)
    }
}

/// Manages writing to a single shard file
struct ShardWriter {
    key: String,
    tmp_path: PathBuf,
    final_path: PathBuf,
    writer: BufWriter<File>,
    row_count: u64,
    byte_size: u64,
    has_header: bool,
    first_source_offset: Option<u64>,
    last_source_offset: u64,
}

impl ShardWriter {
    fn new(key: &str, output_dir: &Path, header: Option<&str>) -> Result<Self, ShredError> {
        let safe_key = sanitize_filename(key);
        let tmp_dir = output_dir.join(".tmp");
        fs::create_dir_all(&tmp_dir)?;

        let tmp_path = tmp_dir.join(format!("{}.csv", safe_key));
        let final_path = output_dir.join(format!("{}.csv", safe_key));

        let file = OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(&tmp_path)?;

        let mut writer = BufWriter::new(file);
        let mut byte_size = 0u64;
        let has_header = header.is_some();

        // Write header if provided
        if let Some(h) = header {
            writeln!(writer, "{}", h)?;
            byte_size += h.len() as u64 + 1;
        }

        Ok(Self {
            key: key.to_string(),
            tmp_path,
            final_path,
            writer,
            row_count: 0,
            byte_size,
            has_header,
            first_source_offset: None,
            last_source_offset: 0,
        })
    }

    fn write_line(&mut self, line: &str, source_offset: u64) -> Result<(), ShredError> {
        writeln!(self.writer, "{}", line)?;
        self.row_count += 1;
        self.byte_size += line.len() as u64 + 1;
        if self.first_source_offset.is_none() {
            self.first_source_offset = Some(source_offset);
        }
        self.last_source_offset = source_offset;
        Ok(())
    }

    fn flush(&mut self) -> Result<(), ShredError> {
        self.writer.flush()?;
        Ok(())
    }

    fn finalize(mut self, _output_dir: &Path) -> Result<ShardMeta, ShredError> {
        self.flush()?;

        // Atomic rename from .tmp to final location
        fs::rename(&self.tmp_path, &self.final_path)?;

        Ok(ShardMeta {
            path: self.final_path,
            key: self.key,
            row_count: self.row_count,
            byte_size: self.byte_size,
            has_header: self.has_header,
            first_source_offset: self.first_source_offset.unwrap_or(0),
            last_source_offset: self.last_source_offset,
        })
    }
}

/// Block-based lineage writer
struct LineageWriter {
    writer: BufWriter<File>,
    current_block: LineageBlock,
    block_threshold: usize,
    bytes_in_block: usize,
}

impl LineageWriter {
    fn new(path: &Path) -> Result<Self, ShredError> {
        let file = OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(path)?;

        Ok(Self {
            writer: BufWriter::new(file),
            current_block: LineageBlock {
                block_id: 0,
                source_offset_start: 0,
                source_offset_end: 0,
                shard_key: String::new(),
                row_count_in_block: 0,
                first_row_number_in_shard: 0,
            },
            block_threshold: LINEAGE_BLOCK_SIZE,
            bytes_in_block: 0,
        })
    }

    fn record(
        &mut self,
        source_offset: u64,
        shard_key: &str,
        line_length: usize,
        shard_row_number: u64,
    ) -> Result<(), ShredError> {
        // If key changed or block full, flush current block
        if (self.current_block.shard_key != shard_key && !self.current_block.shard_key.is_empty())
            || self.bytes_in_block >= self.block_threshold
        {
            self.flush_block()?;
            self.current_block = LineageBlock {
                block_id: self.current_block.block_id + 1,
                source_offset_start: source_offset,
                source_offset_end: source_offset,
                shard_key: shard_key.to_string(),
                row_count_in_block: 0,
                first_row_number_in_shard: shard_row_number,
            };
            self.bytes_in_block = 0;
        }

        // First record in block
        if self.current_block.shard_key.is_empty() {
            self.current_block.shard_key = shard_key.to_string();
            self.current_block.source_offset_start = source_offset;
            self.current_block.first_row_number_in_shard = shard_row_number;
        }

        self.current_block.source_offset_end = source_offset + line_length as u64;
        self.current_block.row_count_in_block += 1;
        self.bytes_in_block += line_length;

        Ok(())
    }

    fn flush_block(&mut self) -> Result<(), ShredError> {
        if self.current_block.row_count_in_block > 0 {
            // Write block as CSV line
            writeln!(
                self.writer,
                "{},{},{},{},{},{}",
                self.current_block.block_id,
                self.current_block.source_offset_start,
                self.current_block.source_offset_end,
                self.current_block.shard_key,
                self.current_block.row_count_in_block,
                self.current_block.first_row_number_in_shard
            )?;
        }
        Ok(())
    }

    fn finalize(mut self) -> Result<(), ShredError> {
        self.flush_block()?;
        self.writer.flush()?;
        Ok(())
    }
}

/// The main Shredder engine
pub struct Shredder {
    config: ShredConfig,
}

impl Shredder {
    pub fn new(config: ShredConfig) -> Self {
        Self { config }
    }

    /// Create with default configuration
    pub fn with_defaults(strategy: ShredStrategy, output_dir: PathBuf) -> Self {
        Self::new(ShredConfig {
            strategy,
            output_dir,
            max_handles: DEFAULT_MAX_HANDLES,
            top_n_shards: DEFAULT_TOP_N_SHARDS,
            buffer_size: DEFAULT_BUFFER_SIZE,
            promotion_threshold: DEFAULT_PROMOTION_THRESHOLD,
        })
    }

    /// Main shred operation - single pass with dynamic promotion
    pub fn shred(&self, input: &Path) -> Result<ShredResult, ShredError> {
        let start_time = std::time::Instant::now();

        // Ensure output directory exists
        fs::create_dir_all(&self.config.output_dir)?;

        // Extract header if CSV with header
        let header = self.extract_header(input)?;

        // Phase 1: Count keys in first pass to determine top-N
        // (For small files, we could do single pass with dynamic promotion,
        // but for clarity we do a simpler two-phase approach)
        let key_counts = self.count_keys(input)?;

        // Determine top-N keys (rest go to freezer)
        let mut sorted_keys: Vec<_> = key_counts.iter().collect();
        sorted_keys.sort_by_key(|(_, count)| std::cmp::Reverse(**count));

        let promoted_keys: std::collections::HashSet<String> = sorted_keys
            .iter()
            .take(self.config.top_n_shards)
            .map(|(k, _)| (*k).clone())
            .collect();

        // Phase 2: Shred with lineage
        let lineage_path = self.config.output_dir.join("lineage.idx");
        let mut lineage_writer = LineageWriter::new(&lineage_path)?;
        let mut writer_cache = WriterCache::new(self.config.max_handles);
        let mut shard_row_counts: HashMap<String, u64> = HashMap::new();

        let file = File::open(input)?;
        let reader = BufReader::with_capacity(self.config.buffer_size, file);

        let mut current_offset: u64 = 0;
        let mut total_rows: u64 = 0;
        let mut is_first_line = true;
        let mut bytes_since_checkpoint: u64 = 0;

        for line_result in reader.lines() {
            let line = line_result?;
            let line_len = line.len();

            // Skip header line if CSV with header
            if is_first_line {
                is_first_line = false;
                if header.is_some() {
                    current_offset += line_len as u64 + 1;
                    continue;
                }
            }

            // Extract key
            let key = self.extract_key(&line)?;

            // Determine destination (promoted key or freezer)
            let dest_key = if promoted_keys.contains(&key) {
                key.clone()
            } else {
                FREEZER_KEY.to_string()
            };

            // Get or create writer
            let writer = writer_cache.get_or_create(
                &dest_key,
                &self.config.output_dir,
                header.as_deref(),
            )?;

            // Track row number in shard
            let shard_row = *shard_row_counts.get(&dest_key).unwrap_or(&0);
            shard_row_counts
                .entry(dest_key.clone())
                .and_modify(|c| *c += 1)
                .or_insert(1);

            // Write line
            writer.write_line(&line, current_offset)?;

            // Record lineage
            lineage_writer.record(current_offset, &dest_key, line_len, shard_row)?;

            current_offset += line_len as u64 + 1;
            total_rows += 1;
            bytes_since_checkpoint += line_len as u64 + 1;

            // Checkpoint if needed (not implemented for now - would write to .checkpoint file)
            if bytes_since_checkpoint >= CHECKPOINT_INTERVAL_BYTES {
                writer_cache.flush_all()?;
                bytes_since_checkpoint = 0;
            }
        }

        // Finalize
        lineage_writer.finalize()?;
        let shards = writer_cache.finalize_all(&self.config.output_dir)?;

        // Count freezer keys
        let freezer_key_count = key_counts.len().saturating_sub(self.config.top_n_shards);
        let freezer_path = if freezer_key_count > 0 {
            Some(self.config.output_dir.join(format!("{}.csv", FREEZER_KEY)))
        } else {
            None
        };

        // Clean up .tmp directory
        let tmp_dir = self.config.output_dir.join(".tmp");
        if tmp_dir.exists() {
            let _ = fs::remove_dir_all(&tmp_dir);
        }

        Ok(ShredResult {
            shards,
            freezer_path,
            freezer_key_count,
            total_rows,
            duration_ms: start_time.elapsed().as_millis() as u64,
            lineage_index_path: lineage_path,
        })
    }

    /// Extract header line if CSV with header
    fn extract_header(&self, path: &Path) -> Result<Option<String>, ShredError> {
        match &self.config.strategy {
            ShredStrategy::CsvColumn { has_header: true, .. } => {
                let file = File::open(path)?;
                let reader = BufReader::new(file);
                if let Some(Ok(line)) = reader.lines().next() {
                    Ok(Some(line))
                } else {
                    Ok(None)
                }
            }
            _ => Ok(None),
        }
    }

    /// Count unique keys in first pass
    fn count_keys(&self, path: &Path) -> Result<HashMap<String, u64>, ShredError> {
        let file = File::open(path)?;
        let reader = BufReader::with_capacity(self.config.buffer_size, file);
        let mut counts: HashMap<String, u64> = HashMap::new();

        let skip_first = matches!(
            &self.config.strategy,
            ShredStrategy::CsvColumn { has_header: true, .. }
        );

        for (i, line_result) in reader.lines().enumerate() {
            if skip_first && i == 0 {
                continue;
            }

            let line = line_result?;
            let key = self.extract_key(&line)?;
            *counts.entry(key).or_insert(0) += 1;
        }

        Ok(counts)
    }

    /// Extract shard key from a line based on strategy
    fn extract_key(&self, line: &str) -> Result<String, ShredError> {
        match &self.config.strategy {
            ShredStrategy::CsvColumn {
                delimiter,
                col_index,
                ..
            } => {
                let delim = *delimiter as char;
                let fields: Vec<&str> = line.split(delim).collect();
                fields
                    .get(*col_index)
                    .map(|s: &&str| s.trim().to_string())
                    .ok_or_else(|| {
                        ShredError::KeyExtraction(format!(
                            "Column {} not found in line",
                            col_index
                        ))
                    })
            }
            ShredStrategy::JsonKey { key_path } => {
                let parsed: serde_json::Value = serde_json::from_str(line).map_err(|e| {
                    ShredError::KeyExtraction(format!("JSON parse error: {}", e))
                })?;

                // Simple single-level key lookup
                parsed
                    .get(key_path)
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string())
                    .ok_or_else(|| {
                        ShredError::KeyExtraction(format!("Key '{}' not found in JSON", key_path))
                    })
            }
            ShredStrategy::Regex { pattern, key_group } => {
                let re = regex::Regex::new(pattern).map_err(|e| {
                    ShredError::KeyExtraction(format!("Invalid regex: {}", e))
                })?;

                re.captures(line)
                    .and_then(|caps| caps.name(key_group))
                    .map(|m| m.as_str().to_string())
                    .ok_or_else(|| {
                        ShredError::KeyExtraction(format!(
                            "Regex group '{}' not matched",
                            key_group
                        ))
                    })
            }
            ShredStrategy::Passthrough => Ok("_ALL".to_string()),
        }
    }
}

/// Sanitize a string for use as a filename
fn sanitize_filename(s: &str) -> String {
    s.chars()
        .map(|c| match c {
            '/' | '\\' | ':' | '*' | '?' | '"' | '<' | '>' | '|' => '_',
            _ => c,
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::Write;
    use tempfile::{tempdir, NamedTempFile};

    fn create_test_csv(rows: usize, types: &[&str]) -> NamedTempFile {
        let mut file = NamedTempFile::new().unwrap();
        writeln!(file, "timestamp,message_type,value").unwrap();
        for i in 0..rows {
            let msg_type = types[i % types.len()];
            writeln!(file, "2024-01-01 00:00:{:02},{},{:.2}", i % 60, msg_type, i as f64).unwrap();
        }
        file.flush().unwrap();
        file
    }

    #[test]
    fn test_basic_shred() {
        let input = create_test_csv(100, &["TYPE_A", "TYPE_B", "TYPE_C"]);
        let output_dir = tempdir().unwrap();

        let shredder = Shredder::with_defaults(
            ShredStrategy::CsvColumn {
                delimiter: b',',
                col_index: 1,
                has_header: true,
            },
            output_dir.path().to_path_buf(),
        );

        let result = shredder.shred(input.path()).unwrap();

        assert_eq!(result.total_rows, 100);
        assert!(result.shards.len() <= 5); // top_n_shards default is 5
        assert!(result.lineage_index_path.exists());

        // Check shards have headers
        for shard in &result.shards {
            assert!(shard.has_header);
            assert!(shard.path.exists());

            // Verify header is correct
            let content = fs::read_to_string(&shard.path).unwrap();
            assert!(content.starts_with("timestamp,message_type,value"));
        }
    }

    #[test]
    fn test_freezer_for_rare_types() {
        // Create file with 3 common types and 10 rare types
        let mut file = NamedTempFile::new().unwrap();
        writeln!(file, "ts,type,val").unwrap();

        // 100 rows of common types
        for i in 0..100 {
            let t = match i % 3 {
                0 => "COMMON_A",
                1 => "COMMON_B",
                _ => "COMMON_C",
            };
            writeln!(file, "{},{},{}", i, t, i).unwrap();
        }

        // 10 rows of rare types (1 each)
        for i in 0..10 {
            writeln!(file, "{},RARE_{},{}", 100 + i, i, i).unwrap();
        }
        file.flush().unwrap();

        let output_dir = tempdir().unwrap();
        let config = ShredConfig {
            strategy: ShredStrategy::CsvColumn {
                delimiter: b',',
                col_index: 1,
                has_header: true,
            },
            output_dir: output_dir.path().to_path_buf(),
            max_handles: 200,
            top_n_shards: 3, // Only top 3 get dedicated files
            buffer_size: 65536,
            promotion_threshold: 1000,
        };

        let shredder = Shredder::new(config);
        let result = shredder.shred(file.path()).unwrap();

        // Should have 3 main shards + 1 freezer
        assert_eq!(result.shards.len(), 4);
        assert!(result.freezer_path.is_some());
        assert!(result.freezer_key_count > 0);

        // Freezer should exist and contain rare types
        let freezer_shard = result.shards.iter().find(|s| s.key == FREEZER_KEY).unwrap();
        assert!(freezer_shard.row_count > 0);
    }

    #[test]
    fn test_lineage_blocks() {
        let input = create_test_csv(50, &["A", "B"]);
        let output_dir = tempdir().unwrap();

        let shredder = Shredder::with_defaults(
            ShredStrategy::CsvColumn {
                delimiter: b',',
                col_index: 1,
                has_header: true,
            },
            output_dir.path().to_path_buf(),
        );

        let result = shredder.shred(input.path()).unwrap();

        // Lineage file should exist
        assert!(result.lineage_index_path.exists());

        // Read and verify lineage format
        let content = fs::read_to_string(&result.lineage_index_path).unwrap();
        let lines: Vec<&str> = content.lines().collect();

        // Should have at least one block
        assert!(!lines.is_empty());

        // Each line should have 6 fields
        for line in lines {
            let fields: Vec<&str> = line.split(',').collect();
            assert_eq!(fields.len(), 6, "Lineage line should have 6 fields: {}", line);
        }
    }

    #[test]
    fn test_json_lines_shred() {
        let mut file = NamedTempFile::new().unwrap();
        for i in 0..30 {
            let event = match i % 3 {
                0 => "login",
                1 => "logout",
                _ => "action",
            };
            writeln!(
                file,
                r#"{{"timestamp":"2024-01-01","event":"{}","id":{}}}"#,
                event, i
            ).unwrap();
        }
        file.flush().unwrap();

        let output_dir = tempdir().unwrap();

        let shredder = Shredder::with_defaults(
            ShredStrategy::JsonKey {
                key_path: "event".to_string(),
            },
            output_dir.path().to_path_buf(),
        );

        let result = shredder.shred(file.path()).unwrap();

        assert_eq!(result.total_rows, 30);
        assert_eq!(result.shards.len(), 3); // login, logout, action
    }

    #[test]
    fn test_atomic_write() {
        let input = create_test_csv(10, &["A"]);
        let output_dir = tempdir().unwrap();

        let shredder = Shredder::with_defaults(
            ShredStrategy::CsvColumn {
                delimiter: b',',
                col_index: 1,
                has_header: true,
            },
            output_dir.path().to_path_buf(),
        );

        let result = shredder.shred(input.path()).unwrap();

        // .tmp directory should be cleaned up
        let tmp_dir = output_dir.path().join(".tmp");
        assert!(!tmp_dir.exists());

        // Final files should exist
        for shard in &result.shards {
            assert!(shard.path.exists());
        }
    }
}
